여러분 
제가 제주도 학회 일정으로 인해서 대회가 종결 될 때까지 모델 개발을 하는데는 조금 힘들 것 같습니다.(미안합니다)
그래도 노트북은 챙겨 갔으니 최대한 남는 시간을 활용해서 아이디어를 생성해 낼 수 있도록 해보겠습니다.

일단 model_DEV_2.ipynb에 해당하는 모델은 0.8 성능을 내는 모델입니다. 사실 어쩌다 보니 만들어졌네요..
대회에 순위권에 오를 만큼 그렇게 좋은 성능을 보이는 모델은 아니지만 조금의 튜닝을 한다면, 더 가치있는 결과를 얻을 수 있을 것으로 보입니다.

그래서 여러분들에게 조금 부탁을 하려고 합니다.(모든 코드에는 주석을 첨부하여 붙여두었습니다.)

⭐️가장 중요한 부분은 모델의 하이퍼파라미터최적화로 생각됩니다.
각 모델 집합은 XGBoost, Randomforest, LGBM을 한개의 세트로 이용해서 예측 결과를 산출하도록 설계되어 있습니다.
그런데, 여기에 들어가 있는 모델들은 각각의 데이터 셋에 대해서 하이퍼파라미터 최적화가 이루어 지지 않았습니다.
(그냥 제가 임의로 n_estimators=2000, learning_rate=0.07, max_depth=16 ... 을 지정하였습니다.)
따라서 최적화 방법(AutoML)을 이용해서 모델을 최적화 해주시면 정말 감사하겠습니다.
(베이지안 최적화란 방법도 있더라구요.. https://dacon.io/competitions/open/235698/talkboard/403915?page=1&dtype=recent)

⭐️다음으로는 특성공학(Feature Engineering)입나다.
모델에게 줄 데이터를 생성하는 과정인데, 여러분이 찾은 "심리관련정보"를 일단 데이터에 적용을 해서 생성한 다음, 모델 훈련을 진행하는 과정이 필요할 것으로 생각됩니다.
따라서 이 부분에 대한 구현이 필요합니다.

그리고 모델에 딥러닝 Linear 모델을 추가해봤는데 성능이 그렇게 좋지는 않더라고요..(혹시 특성공학을 잘 해서 좋은 딥러닝 모델을 만들어서, 또는 다른 방식으로 모델을 생성해서 붙인다면 좋은 성능을 보일 수 있을 것으로 생각됩니다.)
모델을 학습할때는 train 으로 배정된 모든 데이터를 다 사용해야합니다.(train_test_split으로 나누면 모델이 학습하는 데이터의 수에 제한이 생깁니다)

그리고 추가적인 여러가지 여러분들이 구현한 모델들을 붙여주셔도 좋습니다. 아니면, 더 좋은 성능을 띄는 모델을 main으로 개발하는 것도 좋구요!

일단, 제가 만든 모델에 대한 comment는 이 정도 입니다.
더 성능을 올려보려고 했는데 그러지 못한 점이 조금 아쉽네요.

지금은 본선 진출에 대해서 불투명해 보일지라도, 포기하지말고 최강 AI 마지막까지 파이팅🔥
